{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt7vbWcNfB6b"
      },
      "source": [
        "# MBA FIAP Inteligência Artificial & Machine Learning\n",
        "\n",
        "## Visão Computacional: Análise de Imagens Médicas\n",
        "\n",
        "> Atenção: este notebook foi desenhado para funcionar no **Google Collab**.\n",
        "\n",
        "\n",
        "## 1. Introdução\n",
        "\n",
        "Uma determinada fintech focada em consumidores finais pessoa física constataou um grande número de fraudes em transações bancárias.\n",
        "\n",
        "O setor de fraudes apontou que existem clientes que se queixaram de não contratar serviços específicos, como o crédito pessoal, e após isso transferir para outras contas desconhecidas.\n",
        "\n",
        "Após análises pelas equipes de segurança, os protocolos de utilização da senha foram realizados em conformidade, ou seja, cada cliente autenticou com sua própria senha de maneira regular.\n",
        "\n",
        "Em função disso, o banco precisa arcar com reembolsos e medidas de contenção para evitar processos judiciais, pois os clientes alegam terem sido invadidos por hackers ou algo parecido.\n",
        "\n",
        "Uma das formas de solucionar ou minimizar este problema é com a utilização de outras formas de autenticação, sobretudo em operações críticas, como a obtenção de crédito pessoal.\n",
        "\n",
        "Desta forma podemos implementar uma verificação de identidade com prova de vida (liveness), que utilize uma verificação e identificação facial.\n",
        "\n",
        "Caso o cliente não seja autenticado, ele será atendido por uma esteira dedicada e as evidências da não identificação serão encaminhadas para a área de IA para validação dos parâmetros e limiares para aperfeiçoamento do modelo.\n",
        "\n",
        "Será necessário construir:\n",
        "\n",
        "* Detector de faces\n",
        "* Identificação de faces (podendo ser um comparador entre um rosto de documento e outra da prova de vida)\n",
        "* Detecção de vivacidade (liveness) para evitar que um fraudador utilize uma foto estática.\n",
        "\n",
        "\n",
        ">Formas alternativas de prover a identificação e prova de vivacidade, além destas que foram solicitadas poderão ser submetidas.\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://github.com/michelpf/fiap-ml-visao-computacional-detector-liveness/blob/master/notebook/imagens/liveness.jpg?raw=1\">\n",
        "</p>\n",
        "\n",
        "Imagem retirada do [Grunge](https://www.grunge.com/192826/company-testing-robocop-facial-recognition-software-with-us-police/).\n",
        "\n",
        "## 2. Instruções\n",
        "\n",
        "Este projeto final tem como objetivo explorar os conhecimentos adquiridos nas aulas práticas.\n",
        "\n",
        "Iremos constuir uma forma de validar se uma determinada imagem foi ou não adulterada e se trata de uma produção fraudade.\n",
        "\n",
        "Existem diversas formas de validar a vivacidade, e neste sentido conto com a criatividade de vocês dado que já dominam encontrar uma face numa imagem, aplicar marcos faciais e até mesmo construir uma rede neural convulacional.\n",
        "\n",
        "A abordagem mais simples é pela construção de uma rede neural com imagens de fotos de rostos de outras fotos e fotos de rostos sem modificações. Tal classificador deverá classificar se dada imagem possui vivacidade ou não com uma pontuação de probabilidade.\n",
        "\n",
        "Referências que abordam o tema para servir de inspiração:\n",
        "\n",
        "1. [PyImageSearch](https://pyimagesearch.com/2019/03/11/liveness-detection-with-opencv/), Liveness detection with OpenCV;\n",
        "2. [Kickertech](https://kickertech.com/face-liveness-detection-via-opencv-and-tensorflow/), Liveness detection via OpenCV and Tensorflow.\n",
        "3. [Towards Data Science](https://towardsdatascience.com/real-time-face-liveness-detection-with-python-keras-and-opencv-c35dc70dafd3?gi=24f8e1b740f9), Real-time face liveness detection with Python, Keras and OpenCV.\n",
        "\n",
        "Este projeto poderá ser feita por grupos de até 4 pessoas.\n",
        "Caso este projeto seja substitutivo, deverá ser realizado por apenas uma pessoa.\n",
        "\n",
        "| Nome dos Integrantes     | RM             | Turma |\n",
        "| :----------------------- | :------------- | :-----: |\n",
        "| Pedro Ernesto            | RM 347938      | XIA |\n",
        "| Tulio Vivaldini          | RM 348249      | XIA |\n",
        "| Mayara Pardo Martins     | RM 347576      | XIA |\n",
        "| Elton da Silva Pinheiro  | RM 348230      | XIA |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXZ1GI1XfB6f"
      },
      "source": [
        "## 3. Abordagem e organização da solução do problema (2 pontos)\n",
        "\n",
        "Como o grupo pretende deteccar a prova de vivacidade de uma determinada imagem? Quais os passos e os building blocks deste processo?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGjgeskafB6g"
      },
      "source": [
        "**Resposta**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_g7KGfjfB6g"
      },
      "source": [
        "## 4 Desenvolvimento da solução (5,5 pontos)\n",
        "\n",
        "Detalhe o passo-a-passo do algoritmo de deteção de vivacidade.\n",
        "Se optar pela construção e treinamento de um modelo de redes neurais convulucionais, apresente a arquitetura, prepare os dados de treinamento, realize o treinamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1YbpiPNfB6h"
      },
      "source": [
        "### 4.1 Organização de dados para treinamento de modelo de liveness (2 pontos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqDdISqdfB6h"
      },
      "outputs": [],
      "source": [
        "#IMPLEMENTAR\n",
        "#Vamos ler os VIDEOS e separar em FRAMES PARA FAZER O LIVENESS\n",
        "#SAO SEPARADOS APOS O CORTE DE FRAMES EM DUAS PASTAS   ----------IMAGENS_ML_VERDADEIRA E IMAGENS_ML_FALSAS\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "\n",
        "def save_frames_every_second(video_path, output_directory):\n",
        "    # Certifique-se de que o diretório de saída exista\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "    # Caminho completo para o executável do FFmpeg\n",
        "    # Substitua pelo caminho real\n",
        "    ffmpeg_executable = r'C:\\Program Files (x86)\\TubeDigger\\ffmpeg.exe'\n",
        "\n",
        "    # Use subprocess para chamar o comando ffmpeg e extrair frames a cada segundo\n",
        "    command = [\n",
        "        ffmpeg_executable,\n",
        "        '-i', video_path,\n",
        "        '-vf', 'fps=10000',\n",
        "        os.path.join(output_directory, 'frame%d.jpg')\n",
        "    ]\n",
        "    subprocess.run(command)\n",
        "\n",
        "\n",
        "# Exemplo de uso\n",
        "video_path = \"D:\\FILME_ML/15_True.mp4\"\n",
        "output_directory = \"E:\\IMAGENS_ML_VERDADEIRA\"\n",
        "save_frames_every_second(video_path, output_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08s7hEsAfB6i"
      },
      "source": [
        "### 4.2 Treinamento de modelo de liveness (1,5 pontos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nZ2rlm6fB6i"
      },
      "outputs": [],
      "source": [
        "##ESSE TÓPICO ESTA JUNTO COM O PRÓXIMO QUE É O 4.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PPW75lQfB6j"
      },
      "source": [
        "### 4.3 Métricas de desempenho do modelo (2 pontos)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from imutils import paths\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.regularizers import l2"
      ],
      "metadata": {
        "id": "DZY4M696hVbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJFfexzOfB6j"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imutils import paths\n",
        "import cv2\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "import numpy as np\n",
        "\n",
        "# Função para carregar as imagens e rótulos\n",
        "\n",
        "\n",
        "def load_images_and_labels(data_folder):\n",
        "    image_paths = list(paths.list_images(data_folder))\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    for image_path in image_paths:\n",
        "        label = image_path.split(os.path.sep)[-2]\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.resize(image, (32, 32))  # Ajuste conforme necessário\n",
        "        image = img_to_array(image)\n",
        "        data.append(image)\n",
        "        labels.append(label)\n",
        "\n",
        "    return np.array(data), np.array(labels)\n",
        "\n",
        "\n",
        "# Carregar dados de faces reais e faces falsas\n",
        "real_data, real_labels = load_images_and_labels('D:\\IMAGENS_ML_VERDADEIRA')\n",
        "fake_data, fake_labels = load_images_and_labels('D:\\IMAGENS_ML_FALSA')\n",
        "\n",
        "# Pré-processamento e divisão dos dados\n",
        "data = np.vstack([real_data, fake_data])\n",
        "labels = np.hstack([real_labels, fake_labels])\n",
        "le = LabelEncoder().fit(labels)\n",
        "labels = to_categorical(le.transform(labels))\n",
        "(trainX, testX, trainY, testY) = train_test_split(\n",
        "    data, labels, test_size=0.25, random_state=42)\n",
        "\n",
        "# Verificar as dimensões dos dados\n",
        "print(\"Dimensões dos dados de treinamento:\", trainX.shape)\n",
        "print(\"Dimensões dos dados de teste:\", testX.shape)\n",
        "\n",
        "# Inicializar o modelo\n",
        "model = Sequential()\n",
        "\n",
        "# Primeira camada convolucional\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(MaxPooling2D(2, 2))\n",
        "\n",
        "# Segunda camada convolucional\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(2, 2))\n",
        "\n",
        "# Terceira camada convolucional\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(2, 2))\n",
        "\n",
        "# Camada de achatamento (Flatten)\n",
        "model.add(Flatten())\n",
        "\n",
        "# Camada de dropout\n",
        "model.add(Dropout(0.7))\n",
        "\n",
        "# Camada densa com regularização L2\n",
        "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "\n",
        "# Camada de saída\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "# Compilar o modelo\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Treinar o modelo\n",
        "model.fit(trainX, trainY, epochs=5, batch_size=100,\n",
        "          validation_data=(testX, testY))\n",
        "\n",
        "# Avaliar a acurácia no conjunto de teste\n",
        "predictions = model.predict(testX, batch_size=100)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "true_labels = np.argmax(testY, axis=1)\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(f'Acurácia: {accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQTH23HPfB6j"
      },
      "source": [
        "## 5 Teste Fim-a-Fim\n",
        "\n",
        "Simule a operação fim-a-fim, com uma imagem de entrada forjada (foto de foto de um rosto) e outra com uma imagem de rosto, exibindo o resultado da classificação e a pontuação de cada classe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-FNP4JgfB6k"
      },
      "outputs": [],
      "source": [
        "#IMPLEMENTAR\n",
        "############# TESTE CLASSIFICADOR ##############################\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import numpy as np\n",
        "\n",
        "# Corrigir o caminho da imagem\n",
        "# Substitua pelo caminho real da sua imagem\n",
        "image_path = \"D:\\IMAGEM_ML_TESTE\\michelzao.jpg\"\n",
        "\n",
        "# Mapear os rótulos de classe para nomes (verdadeira e falsa)\n",
        "class_names = {0: 'Verdadeira', 1: 'Falso'}\n",
        "\n",
        "# Carregar a imagem que você deseja testar\n",
        "try:\n",
        "    # Ajuste o tamanho conforme necessário\n",
        "    img = load_img(image_path, target_size=(32, 32))\n",
        "    img_array = img_to_array(img)\n",
        "    # Adicionar dimensão extra para o batch\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # Normalizar os valores dos pixels para estar no intervalo [0, 1]\n",
        "    img_array /= 255.0\n",
        "\n",
        "    # Fazer a previsão usando o modelo treinado\n",
        "    predictions = model.predict(img_array)\n",
        "\n",
        "    # Obter a classe prevista\n",
        "    predicted_class = np.argmax(predictions)\n",
        "\n",
        "    # Exibir a classe prevista, a probabilidade associada e o nome da classe\n",
        "    print(f\"Classe prevista: {predicted_class}\")\n",
        "    print(f\"Probabilidade: {predictions[0][predicted_class]}\")\n",
        "    print(f\"Classe: {class_names[predicted_class]}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"A imagem não foi encontrada no caminho especificado: {image_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Ocorreu um erro ao processar a imagem: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7Sd965zfB6k"
      },
      "source": [
        ">Com a implementação da solução na forma de uma aplicação do [Streamlit](https://www.streamlit.io/) (veja a pata streamlit-app e use o template) vale 1 ponto adicional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZbbOu6CfB6k"
      },
      "source": [
        "**Pergunta**: Se utilizou o Streamlit, compartilhe a URL do aplicativo publicado:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDUzqyRhfB6k"
      },
      "source": [
        "**Resposta**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcnlCyldfB6k"
      },
      "source": [
        "## 6 Conclusões (2,5 pontos)\n",
        "\n",
        "**Pergunta**: Dado todo o estudo e pesquisa, quais foram as conclusões sobre a solução, o que funcionou, o que não funcionou e quais os detalhes que observariam numa nova versão e melhorias do processo?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plM29PKLfB6k"
      },
      "source": [
        "**Resposta**: O devido exercício de pesquisa foi desafiador para o acadêmico, pois alem de realizar a divisão dos frames para ter várias imagens. Teve todo o entendimento sobre parte de redes neurais e seus desafios para ajustes. O que funcionou muito bem foi a utilização das redes simples, o que talvez faria sentido melhorar é utilizar redes recorrentes e fazer suas inferências para uma versão e melhoria do processo."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8GbU9Gf_iJmC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.13 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "733a071da2455ea0e8bdf5409a7097e630ac701195faf55c6e985d77ee3ec176"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}